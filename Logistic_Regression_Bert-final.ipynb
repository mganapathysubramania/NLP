{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/NLP/Assignment4/handout/data\n"
     ]
    }
   ],
   "source": [
    "ORIGINAL_DATA_DIR = \"C:/NLP/Assignment4/handout/data\"\n",
    "print(ORIGINAL_DATA_DIR)\n",
    "BERT_FEATURE_DIR = \"C:/NLP/Assignment4/handout/bert_output_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format training data\n",
    "\n",
    "`X` will be a matrix with `N` rows for the `N` texts in the training data, and `M` columns for the `M` features generated by BERT.\n",
    "\n",
    "`y` will be an array of `N` class labels for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(ORIGINAL_DATA_DIR+\"/lang_id_train.csv\")\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(ORIGINAL_DATA_DIR+\"/lang_id_test.csv\")\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_vectors = []\n",
    "with open(os.path.join(BERT_FEATURE_DIR, \"train.jsonlines\"), \"rt\") as infile:\n",
    "    for line in infile:\n",
    "        bert_data = json.loads(line)\n",
    "        for t in bert_data[\"features\"]:\n",
    "            # Only extract the [CLS] vector used for classification\n",
    "            if t[\"token\"] == \"[CLS]\":\n",
    "                # We only use the representation at the final layer of the network\n",
    "                bert_vectors.append(t[\"layers\"][0][\"values\"])\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_vectors_test = []\n",
    "with open(os.path.join(BERT_FEATURE_DIR, \"test.jsonlines\"), \"rt\") as infile:\n",
    "    for line in infile:\n",
    "        bert_data = json.loads(line)\n",
    "        for t in bert_data[\"features\"]:\n",
    "            # Only extract the [CLS] vector used for classification\n",
    "            if t[\"token\"] == \"[CLS]\":\n",
    "                # We only use the representation at the final layer of the network\n",
    "                bert_vectors_test.append(t[\"layers\"][0][\"values\"])\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_vectors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(bert_vectors)\n",
    "y_train = train_df[\"native_language\"].values\n",
    "X_test = np.array(bert_vectors_test)\n",
    "y_test = test_df[\"native_language\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr_model = LogisticRegression(penalty=\"l2\", C=1.0)\n",
    "lr_model = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01556567 -0.05973345 -0.06967004 -0.00044624  0.06709756 -0.09168432\n",
      "  0.01396779  0.11227561 -0.06283247  0.07545989] [[-0.13366956  0.39882331  0.11291944 ... -0.24314519  0.20492511\n",
      "   0.61365363]\n",
      " [-0.46007311 -0.04856964 -0.05859457 ... -0.14061645 -0.00399913\n",
      "  -0.47984901]\n",
      " [ 0.35134614 -0.36917708 -0.10856648 ...  0.26340266 -0.18938361\n",
      "  -0.18184642]\n",
      " ...\n",
      " [-0.34413203  0.07397263  0.0170779  ... -0.04083112  0.55463192\n",
      "   0.25435844]\n",
      " [ 0.11415463 -0.11661916  0.26020379 ... -0.02403476 -0.17154475\n",
      "   0.07271964]\n",
      " [ 0.05855351  0.34197147 -0.06833372 ...  0.551914   -0.06102568\n",
      "   0.3211048 ]]\n"
     ]
    }
   ],
   "source": [
    "print (lr_model.intercept_, lr_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mandarin' 'Arabic' 'Mandarin' ... 'Korean' 'Arabic' 'Spanish']\n"
     ]
    }
   ],
   "source": [
    "pred = lr_model.predict(X_test)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value\n",
      "[[1.58818181e-01 2.22267798e-01 1.63830662e-01 ... 8.01894510e-02\n",
      "  3.28700979e-02 3.67867490e-02]\n",
      " [5.06554824e-01 8.38005974e-02 3.23352467e-02 ... 2.77086336e-02\n",
      "  2.75934674e-02 1.03136888e-01]\n",
      " [1.44538315e-02 1.82150211e-01 3.93920859e-02 ... 4.22845634e-02\n",
      "  5.18522641e-02 1.35692378e-01]\n",
      " ...\n",
      " [1.39229905e-01 2.10166117e-02 8.98570865e-02 ... 9.67388382e-02\n",
      "  1.24187635e-01 1.37639416e-01]\n",
      " [7.68490844e-01 1.78718054e-02 1.29022054e-04 ... 4.97035645e-02\n",
      "  1.53304002e-01 6.04751532e-03]\n",
      " [2.23888484e-01 1.88436804e-02 3.35418894e-03 ... 4.23673565e-01\n",
      "  1.68430544e-02 2.49162461e-01]]\n"
     ]
    }
   ],
   "source": [
    "Prob_value=lr_model.predict_proba(X_test)\n",
    "print(\"Predicted Value\")\n",
    "print(Prob_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.459\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy : \",lr_model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[ 99   6   6  11  14  14  11  25   5   9]\n",
      " [  9  68  16  16  37  12   5   7  15  15]\n",
      " [ 17  10  96  21  14  11   9   6   8   8]\n",
      " [  6  23  25  82  13   9  10   5  13  14]\n",
      " [ 12  49  16  16  63   3  15   8   5  13]\n",
      " [  9  13   5   6   3  99  32  16   7  10]\n",
      " [  7   7  13  10  10  21 110  13   0   9]\n",
      " [ 18   7  11   5  12  16  15 103   3  10]\n",
      " [ 14  12  10  10   8   3   0  12 114  17]\n",
      " [ 16  18  12  10  13  16  10  11  10  84]]\n"
     ]
    }
   ],
   "source": [
    "results = confusion_matrix(y_test,pred)\n",
    "print('Confusion Matrix :')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 0.459\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy Score :',accuracy_score(y_test,pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.48      0.49      0.49       200\n",
      "   Cantonese       0.32      0.34      0.33       200\n",
      "    Japanese       0.46      0.48      0.47       200\n",
      "      Korean       0.44      0.41      0.42       200\n",
      "    Mandarin       0.34      0.32      0.33       200\n",
      "      Polish       0.49      0.49      0.49       200\n",
      "     Russian       0.51      0.55      0.53       200\n",
      "     Spanish       0.50      0.52      0.51       200\n",
      "        Thai       0.63      0.57      0.60       200\n",
      "  Vietnamese       0.44      0.42      0.43       200\n",
      "\n",
      "    accuracy                           0.46      2000\n",
      "   macro avg       0.46      0.46      0.46      2000\n",
      "weighted avg       0.46      0.46      0.46      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Report : ')\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score -Macro     -  0.459037371220077\n",
      "F1 Score - Micro    -  0.459\n",
      "F1 Score - Weighted -  0.45903737122007704\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-Score -Macro     - \", f1_score(y_test, pred, average='macro'))  \n",
    "print(\"F1 Score - Micro    - \", f1_score(y_test, pred, average='micro'))\n",
    "print(\"F1 Score - Weighted - \", f1_score(y_test, pred, average='weighted')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score - Macro    -  0.4600037960339021\n",
      "precision score - Micro    -  0.459\n",
      "precision score - Weighted -  0.4600037960339021\n"
     ]
    }
   ],
   "source": [
    "print(\"precision score - Macro    - \",precision_score(y_test, pred, average='macro'))\n",
    "print(\"precision score - Micro    - \",precision_score(y_test, pred, average='micro'))\n",
    "print(\"precision score - Weighted - \",precision_score(y_test, pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Score - Macro    -  0.45899999999999996\n",
      "Recall Score - Micro    -  0.459\n",
      "Recall Score - Weighted -  0.459\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall Score - Macro    - \",recall_score(y_test, pred, average='macro'))\n",
    "print(\"Recall Score - Micro    - \",recall_score(y_test, pred, average='micro'))\n",
    "print(\"Recall Score - Weighted - \",recall_score(y_test, pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
